{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Prepare Exercises\n",
    "\n",
    "The end result of this exercise should be a file named ```prepare.py``` that defines the requested functions.\n",
    "\n",
    "In this exercise we will be defining some functions to prepare textual data. These functions should apply equally well to both the codeup blog articles and the news articles that were previously acquired.\n",
    "\n",
    "**1. Define a function named ```basic_clean```. It should take in a string and apply some basic text cleaning to it:**\n",
    "- Lowercase everything\n",
    "- Normalize unicode characters\n",
    "- Replace anything that is not a letter, number, whitespace or a single quote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "\n",
    "import acquire\n",
    "import prepare\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(text):\n",
    "    '''\n",
    "    This function takes in a string and normalizes it by lowercasing\n",
    "    everything and replacing anything that is not a letter, number, \n",
    "    whitespace or a single quote.\n",
    "    '''\n",
    "    \n",
    "    #lowercase all letters in the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # normalize unicode by encoding into ASCII (ignore non-ASCII characters)\n",
    "    # then decoding back into unicode \n",
    "    text = unicodedata.normalize('NFKD', text)\\\n",
    "    .encode('ascii', 'ignore')\\\n",
    "    .decode('utf-8', 'ignore')\n",
    "\n",
    "    # remove any that is not a letter, number, single quote, or whitespace\n",
    "    text = re.sub(r\"[^a-z0-9'\\s]\", '', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"HERE is a sTring with lőt^^^S óf s\\\\##tranGe things+ go%$ing ón. I'm attempting to 'normalize' some text.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"HERE is a sTring with lőt^^^S óf s\\\\##tranGe things+ go%$ing ón. I'm attempting to 'normalize' some text.\"\n",
    "text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"here is a string with lots of strange things going on i'm attempting to 'normalize' some text\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = basic_clean(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.  Define a function named ```tokenize```. It should take in a string and tokenize all the words in the string.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    '''\n",
    "    This function takes in a string and returns the string will the\n",
    "    words tokenized\n",
    "    '''\n",
    "\n",
    "    # Create the tokenizer\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "\n",
    "    # Use the tokenizer\n",
    "    text = tokenizer.tokenize(text, return_str=True)\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"here is a string with lots of strange things going on i ' m attempting to ' normalize ' some text\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = tokenize(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.  Define a function named ```stem```. It should accept some text and return the text after applying stemming to all the words.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(text):\n",
    "    '''\n",
    "    This function takes in a string and returns the string after applying\n",
    "    stemming to all the words.\n",
    "    '''\n",
    "\n",
    "    # Create the porter stemmer\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "\n",
    "    # Apply the stemmer to each word in our string.\n",
    "    stems = [ps.stem(word) for word in text.split()]\n",
    "    \n",
    "    # Join our lists of words into a string again\n",
    "    text_stemmed = ' '.join(stems)\n",
    "\n",
    "    return text_stemmed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"here is a string with lot of strang thing go on i ' m attempt to ' normal ' some text\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Define a function named ```lemmatize```. It should accept some text and return the text after applying lemmatization to each word.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    '''\n",
    "    This function takes in a string and returns the string after applying\n",
    "    lemmatization to all the words.\n",
    "    '''\n",
    "\n",
    "    # Create the Lemmatizer.\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "    # Use the lemmatizer on each word in the list of words we created by using split.\n",
    "    lemmas = [wnl.lemmatize(word) for word in text.split()]\n",
    "\n",
    "    # Join our list of words into a string again; assign to a variable to save changes.\n",
    "    text_lemmatized = ' '.join(lemmas)\n",
    "    \n",
    "    return text_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"here is a string with lot of strange thing going on i ' m attempting to ' normalize ' some text\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = lemmatize(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Define a function named ```remove_stopwords```. It should accept some text and return the text after removing all the stopwords.**\n",
    "\n",
    "This function should define two optional parameters, extra_words and exclude_words. These parameters should define any additional stop words to include, and any words that we don't want to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import standard English language stopwords list from nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(text, extra_words=[], exclude_words=[]):\n",
    "    '''\n",
    "    This function takes in a string and optional lists of extra_words and \n",
    "    words to exclude from the list and then returns the string after removing stop_words\n",
    "    '''\n",
    "\n",
    "    # Define the stop word list\n",
    "    stopword_list = stopwords.words('english')\n",
    "\n",
    "    # add extra_words (if any) to the stopwords list\n",
    "    if len(extra_words) > 0:\n",
    "        stopword_list = stopword_list.append(extra_words)\n",
    "      \n",
    "    # remove exclude_words (if any) from the stopwords list\n",
    "    if len(exclude_words) > 0:\n",
    "        stopword_list = stopword_list.remove(exclude_words)   \n",
    "\n",
    "    # Split words in text.\n",
    "    text = text.split()\n",
    "    \n",
    "    # Create a list of words from my string with stopwords removed and assign to variable.\n",
    "    filtered_words = [word for word in text if word not in stopword_list]\n",
    "    \n",
    "    # Join words in the list back into strings; assign to a variable to keep changes.\n",
    "    text_without_stopwords = ' '.join(filtered_words)\n",
    "\n",
    "    return text_without_stopwords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"string lot strange thing going ' attempting ' normalize ' text\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Use your data from the acquire to produce a dataframe of the news articles. Name the dataframe ```news_df```.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/barbmarques/codeup-data-science/natural-language-processing-exercises/acquire.py:92: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 92 of the file /Users/barbmarques/codeup-data-science/natural-language-processing-exercises/acquire.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  soup = BeautifulSoup(response.text)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Air India pilots demand vaccination on priorit...</td>\n",
       "      <td>Indian Commercial Pilots Association (ICPA) on...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>India underestimated the coronavirus: Raghuram...</td>\n",
       "      <td>Speaking about India's second COVID-19 wave, f...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>World's biggest jeweller says it will no longe...</td>\n",
       "      <td>Pandora, the world's biggest jeweller, has sai...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>South Korea's richest woman gets fortune worth...</td>\n",
       "      <td>South Korea’s richest woman Hong Ra-hee added ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Samsung pledges ₹37 crore to India to fight CO...</td>\n",
       "      <td>Samsung has pledged $5 million (around ₹37 cro...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>Prez Biden raises US' annual refugee admission...</td>\n",
       "      <td>US President Joe Biden has raised the maximum ...</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>Myanmar's military govt bans satellite TV citi...</td>\n",
       "      <td>Myanmar's military government has announced a ...</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>Egypt buys 30 more Rafale jets from France in ...</td>\n",
       "      <td>Egypt will buy 30 more Rafale fighter jets fro...</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Further violence in Myanmar could lead to civi...</td>\n",
       "      <td>China's Ambassador to the UN Zhang Jun on Mond...</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>Could abandon Myanmar project if found to viol...</td>\n",
       "      <td>Adani Ports and Special Economic Zone (APSEZ) ...</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "0    Air India pilots demand vaccination on priorit...   \n",
       "1    India underestimated the coronavirus: Raghuram...   \n",
       "2    World's biggest jeweller says it will no longe...   \n",
       "3    South Korea's richest woman gets fortune worth...   \n",
       "4    Samsung pledges ₹37 crore to India to fight CO...   \n",
       "..                                                 ...   \n",
       "142  Prez Biden raises US' annual refugee admission...   \n",
       "143  Myanmar's military govt bans satellite TV citi...   \n",
       "144  Egypt buys 30 more Rafale jets from France in ...   \n",
       "145  Further violence in Myanmar could lead to civi...   \n",
       "146  Could abandon Myanmar project if found to viol...   \n",
       "\n",
       "                                               content  category  \n",
       "0    Indian Commercial Pilots Association (ICPA) on...  business  \n",
       "1    Speaking about India's second COVID-19 wave, f...  business  \n",
       "2    Pandora, the world's biggest jeweller, has sai...  business  \n",
       "3    South Korea’s richest woman Hong Ra-hee added ...  business  \n",
       "4    Samsung has pledged $5 million (around ₹37 cro...  business  \n",
       "..                                                 ...       ...  \n",
       "142  US President Joe Biden has raised the maximum ...     world  \n",
       "143  Myanmar's military government has announced a ...     world  \n",
       "144  Egypt will buy 30 more Rafale fighter jets fro...     world  \n",
       "145  China's Ambassador to the UN Zhang Jun on Mond...     world  \n",
       "146  Adani Ports and Special Economic Zone (APSEZ) ...     world  \n",
       "\n",
       "[147 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles = acquire.acquire_news_articles()\n",
    "articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.  Make another dataframe for the Codeup blog posts. Name the dataframe ```codeup_df```.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/barbmarques/codeup-data-science/natural-language-processing-exercises/acquire.py:16: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 16 of the file /Users/barbmarques/codeup-data-science/natural-language-processing-exercises/acquire.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  soup = BeautifulSoup(response.text)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>published_date</th>\n",
       "      <th>blog_image</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Codeup’s Data Science Career Accelerator is Here!</td>\n",
       "      <td>September 30, 2018</td>\n",
       "      <td>https://codeup.com/wp-content/uploads/2018/10/...</td>\n",
       "      <td>The rumors are true! The time has arrived. Cod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Science Myths</td>\n",
       "      <td>October 31, 2018</td>\n",
       "      <td>https://codeup.com/wp-content/uploads/2018/10/...</td>\n",
       "      <td>By Dimitri Antoniou and Maggie Giust\\nData Sci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Science VS Data Analytics: What’s The Dif...</td>\n",
       "      <td>October 17, 2018</td>\n",
       "      <td>https://codeup.com/wp-content/uploads/2018/10/...</td>\n",
       "      <td>By Dimitri Antoniou\\nA week ago, Codeup launch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10 Tips to Crush It at the SA Tech Job Fair</td>\n",
       "      <td>August 14, 2018</td>\n",
       "      <td>None</td>\n",
       "      <td>SA Tech Job Fair\\nThe third bi-annual San Anto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Competitor Bootcamps Are Closing. Is the Model...</td>\n",
       "      <td>August 14, 2018</td>\n",
       "      <td>None</td>\n",
       "      <td>Competitor Bootcamps Are Closing. Is the Model...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title      published_date  \\\n",
       "0  Codeup’s Data Science Career Accelerator is Here!  September 30, 2018   \n",
       "1                                 Data Science Myths    October 31, 2018   \n",
       "2  Data Science VS Data Analytics: What’s The Dif...    October 17, 2018   \n",
       "3        10 Tips to Crush It at the SA Tech Job Fair     August 14, 2018   \n",
       "4  Competitor Bootcamps Are Closing. Is the Model...     August 14, 2018   \n",
       "\n",
       "                                          blog_image  \\\n",
       "0  https://codeup.com/wp-content/uploads/2018/10/...   \n",
       "1  https://codeup.com/wp-content/uploads/2018/10/...   \n",
       "2  https://codeup.com/wp-content/uploads/2018/10/...   \n",
       "3                                               None   \n",
       "4                                               None   \n",
       "\n",
       "                                             content  \n",
       "0  The rumors are true! The time has arrived. Cod...  \n",
       "1  By Dimitri Antoniou and Maggie Giust\\nData Sci...  \n",
       "2  By Dimitri Antoniou\\nA week ago, Codeup launch...  \n",
       "3  SA Tech Job Fair\\nThe third bi-annual San Anto...  \n",
       "4  Competitor Bootcamps Are Closing. Is the Model...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# acquire the dataframe of codeup blog articles\n",
    "blog = acquire.acquire_codeup_blog()\n",
    "blog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.  For each dataframe, produce the following columns:**\n",
    "- ```title``` to hold the title\n",
    "- ```original``` to hold the original article/post content\n",
    "- ```clean``` to hold the normalized and tokenized original with the stopwords removed.\n",
    "- ```stemmed``` to hold the stemmed version of the cleaned data.\n",
    "- ```lemmatized``` to hold the lemmatized version of the cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_article_data(df, column, extra_words=[], exclude_words=[]):\n",
    "    '''\n",
    "    This function take in a df and the string name for a text column with \n",
    "    option to pass lists for extra_words and exclude_words and\n",
    "    returns a df with the text article title, original text, stemmed text,\n",
    "    lemmatized text, cleaned, tokenized, & lemmatized text with stopwords removed.\n",
    "    '''\n",
    "    df['clean'] = df[column].apply(basic_clean)\\\n",
    "                            .apply(tokenize)\\\n",
    "                            .apply(remove_stopwords, \n",
    "                                   extra_words=extra_words, \n",
    "                                   exclude_words=exclude_words)\\\n",
    "                            .apply(lemmatize)\n",
    "    \n",
    "    df['stemmed'] = df[column].apply(basic_clean).apply(stem)\n",
    "    \n",
    "    df['lemmatized'] = df[column].apply(basic_clean).apply(lemmatize)\n",
    "    \n",
    "    return df[['title', column, 'stemmed', 'lemmatized', 'clean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Air India pilots demand vaccination on priorit...</td>\n",
       "      <td>Indian Commercial Pilots Association (ICPA) on...</td>\n",
       "      <td>indian commerci pilot associ icpa on tuesday s...</td>\n",
       "      <td>indian commercial pilot association icpa on tu...</td>\n",
       "      <td>indian commercial pilot association icpa tuesd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>India underestimated the coronavirus: Raghuram...</td>\n",
       "      <td>Speaking about India's second COVID-19 wave, f...</td>\n",
       "      <td>speak about india' second covid19 wave former ...</td>\n",
       "      <td>speaking about india's second covid19 wave for...</td>\n",
       "      <td>speaking india ' second covid19 wave former rb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>World's biggest jeweller says it will no longe...</td>\n",
       "      <td>Pandora, the world's biggest jeweller, has sai...</td>\n",
       "      <td>pandora the world' biggest jewel ha said that ...</td>\n",
       "      <td>pandora the world's biggest jeweller ha said t...</td>\n",
       "      <td>pandora world ' biggest jeweller said ' stop u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>South Korea's richest woman gets fortune worth...</td>\n",
       "      <td>South Korea’s richest woman Hong Ra-hee added ...</td>\n",
       "      <td>south korea richest woman hong rahe ad anoth 7...</td>\n",
       "      <td>south korea richest woman hong rahee added ano...</td>\n",
       "      <td>south korea richest woman hong rahee added ano...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Samsung pledges ₹37 crore to India to fight CO...</td>\n",
       "      <td>Samsung has pledged $5 million (around ₹37 cro...</td>\n",
       "      <td>samsung ha pledg 5 million around 37 crore to ...</td>\n",
       "      <td>samsung ha pledged 5 million around 37 crore t...</td>\n",
       "      <td>samsung pledged 5 million around 37 crore help...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>Prez Biden raises US' annual refugee admission...</td>\n",
       "      <td>US President Joe Biden has raised the maximum ...</td>\n",
       "      <td>us presid joe biden ha rais the maximum number...</td>\n",
       "      <td>u president joe biden ha raised the maximum nu...</td>\n",
       "      <td>u president joe biden raised maximum number re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>Myanmar's military govt bans satellite TV citi...</td>\n",
       "      <td>Myanmar's military government has announced a ...</td>\n",
       "      <td>myanmar' militari govern ha announc a ban on s...</td>\n",
       "      <td>myanmar's military government ha announced a b...</td>\n",
       "      <td>myanmar ' military government announced ban sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>Egypt buys 30 more Rafale jets from France in ...</td>\n",
       "      <td>Egypt will buy 30 more Rafale fighter jets fro...</td>\n",
       "      <td>egypt will buy 30 more rafal fighter jet from ...</td>\n",
       "      <td>egypt will buy 30 more rafale fighter jet from...</td>\n",
       "      <td>egypt buy 30 rafale fighter jet france 4 billi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Further violence in Myanmar could lead to civi...</td>\n",
       "      <td>China's Ambassador to the UN Zhang Jun on Mond...</td>\n",
       "      <td>china' ambassador to the un zhang jun on monda...</td>\n",
       "      <td>china's ambassador to the un zhang jun on mond...</td>\n",
       "      <td>china ' ambassador un zhang jun monday said vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>Could abandon Myanmar project if found to viol...</td>\n",
       "      <td>Adani Ports and Special Economic Zone (APSEZ) ...</td>\n",
       "      <td>adani port and special econom zone apsez said ...</td>\n",
       "      <td>adani port and special economic zone apsez sai...</td>\n",
       "      <td>adani port special economic zone apsez said co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "0    Air India pilots demand vaccination on priorit...   \n",
       "1    India underestimated the coronavirus: Raghuram...   \n",
       "2    World's biggest jeweller says it will no longe...   \n",
       "3    South Korea's richest woman gets fortune worth...   \n",
       "4    Samsung pledges ₹37 crore to India to fight CO...   \n",
       "..                                                 ...   \n",
       "142  Prez Biden raises US' annual refugee admission...   \n",
       "143  Myanmar's military govt bans satellite TV citi...   \n",
       "144  Egypt buys 30 more Rafale jets from France in ...   \n",
       "145  Further violence in Myanmar could lead to civi...   \n",
       "146  Could abandon Myanmar project if found to viol...   \n",
       "\n",
       "                                               content  \\\n",
       "0    Indian Commercial Pilots Association (ICPA) on...   \n",
       "1    Speaking about India's second COVID-19 wave, f...   \n",
       "2    Pandora, the world's biggest jeweller, has sai...   \n",
       "3    South Korea’s richest woman Hong Ra-hee added ...   \n",
       "4    Samsung has pledged $5 million (around ₹37 cro...   \n",
       "..                                                 ...   \n",
       "142  US President Joe Biden has raised the maximum ...   \n",
       "143  Myanmar's military government has announced a ...   \n",
       "144  Egypt will buy 30 more Rafale fighter jets fro...   \n",
       "145  China's Ambassador to the UN Zhang Jun on Mond...   \n",
       "146  Adani Ports and Special Economic Zone (APSEZ) ...   \n",
       "\n",
       "                                               stemmed  \\\n",
       "0    indian commerci pilot associ icpa on tuesday s...   \n",
       "1    speak about india' second covid19 wave former ...   \n",
       "2    pandora the world' biggest jewel ha said that ...   \n",
       "3    south korea richest woman hong rahe ad anoth 7...   \n",
       "4    samsung ha pledg 5 million around 37 crore to ...   \n",
       "..                                                 ...   \n",
       "142  us presid joe biden ha rais the maximum number...   \n",
       "143  myanmar' militari govern ha announc a ban on s...   \n",
       "144  egypt will buy 30 more rafal fighter jet from ...   \n",
       "145  china' ambassador to the un zhang jun on monda...   \n",
       "146  adani port and special econom zone apsez said ...   \n",
       "\n",
       "                                            lemmatized  \\\n",
       "0    indian commercial pilot association icpa on tu...   \n",
       "1    speaking about india's second covid19 wave for...   \n",
       "2    pandora the world's biggest jeweller ha said t...   \n",
       "3    south korea richest woman hong rahee added ano...   \n",
       "4    samsung ha pledged 5 million around 37 crore t...   \n",
       "..                                                 ...   \n",
       "142  u president joe biden ha raised the maximum nu...   \n",
       "143  myanmar's military government ha announced a b...   \n",
       "144  egypt will buy 30 more rafale fighter jet from...   \n",
       "145  china's ambassador to the un zhang jun on mond...   \n",
       "146  adani port and special economic zone apsez sai...   \n",
       "\n",
       "                                                 clean  \n",
       "0    indian commercial pilot association icpa tuesd...  \n",
       "1    speaking india ' second covid19 wave former rb...  \n",
       "2    pandora world ' biggest jeweller said ' stop u...  \n",
       "3    south korea richest woman hong rahee added ano...  \n",
       "4    samsung pledged 5 million around 37 crore help...  \n",
       "..                                                 ...  \n",
       "142  u president joe biden raised maximum number re...  \n",
       "143  myanmar ' military government announced ban sa...  \n",
       "144  egypt buy 30 rafale fighter jet france 4 billi...  \n",
       "145  china ' ambassador un zhang jun monday said vi...  \n",
       "146  adani port special economic zone apsez said co...  \n",
       "\n",
       "[147 rows x 5 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_article_data(articles,'content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Codeup’s Data Science Career Accelerator is Here!</td>\n",
       "      <td>The rumors are true! The time has arrived. Cod...</td>\n",
       "      <td>the rumor are true the time ha arriv codeup ha...</td>\n",
       "      <td>the rumor are true the time ha arrived codeup ...</td>\n",
       "      <td>rumor true time arrived codeup officially open...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Science Myths</td>\n",
       "      <td>By Dimitri Antoniou and Maggie Giust\\nData Sci...</td>\n",
       "      <td>by dimitri antoni and maggi giust data scienc ...</td>\n",
       "      <td>by dimitri antoniou and maggie giust data scie...</td>\n",
       "      <td>dimitri antoniou maggie giust data science big...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Science VS Data Analytics: What’s The Dif...</td>\n",
       "      <td>By Dimitri Antoniou\\nA week ago, Codeup launch...</td>\n",
       "      <td>by dimitri antoni a week ago codeup launch our...</td>\n",
       "      <td>by dimitri antoniou a week ago codeup launched...</td>\n",
       "      <td>dimitri antoniou week ago codeup launched imme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10 Tips to Crush It at the SA Tech Job Fair</td>\n",
       "      <td>SA Tech Job Fair\\nThe third bi-annual San Anto...</td>\n",
       "      <td>sa tech job fair the third biannual san antoni...</td>\n",
       "      <td>sa tech job fair the third biannual san antoni...</td>\n",
       "      <td>sa tech job fair third biannual san antonio te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Competitor Bootcamps Are Closing. Is the Model...</td>\n",
       "      <td>Competitor Bootcamps Are Closing. Is the Model...</td>\n",
       "      <td>competitor bootcamp are close is the model in ...</td>\n",
       "      <td>competitor bootcamps are closing is the model ...</td>\n",
       "      <td>competitor bootcamps closing model danger prog...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Codeup’s Data Science Career Accelerator is Here!   \n",
       "1                                 Data Science Myths   \n",
       "2  Data Science VS Data Analytics: What’s The Dif...   \n",
       "3        10 Tips to Crush It at the SA Tech Job Fair   \n",
       "4  Competitor Bootcamps Are Closing. Is the Model...   \n",
       "\n",
       "                                             content  \\\n",
       "0  The rumors are true! The time has arrived. Cod...   \n",
       "1  By Dimitri Antoniou and Maggie Giust\\nData Sci...   \n",
       "2  By Dimitri Antoniou\\nA week ago, Codeup launch...   \n",
       "3  SA Tech Job Fair\\nThe third bi-annual San Anto...   \n",
       "4  Competitor Bootcamps Are Closing. Is the Model...   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  the rumor are true the time ha arriv codeup ha...   \n",
       "1  by dimitri antoni and maggi giust data scienc ...   \n",
       "2  by dimitri antoni a week ago codeup launch our...   \n",
       "3  sa tech job fair the third biannual san antoni...   \n",
       "4  competitor bootcamp are close is the model in ...   \n",
       "\n",
       "                                          lemmatized  \\\n",
       "0  the rumor are true the time ha arrived codeup ...   \n",
       "1  by dimitri antoniou and maggie giust data scie...   \n",
       "2  by dimitri antoniou a week ago codeup launched...   \n",
       "3  sa tech job fair the third biannual san antoni...   \n",
       "4  competitor bootcamps are closing is the model ...   \n",
       "\n",
       "                                               clean  \n",
       "0  rumor true time arrived codeup officially open...  \n",
       "1  dimitri antoniou maggie giust data science big...  \n",
       "2  dimitri antoniou week ago codeup launched imme...  \n",
       "3  sa tech job fair third biannual san antonio te...  \n",
       "4  competitor bootcamps closing model danger prog...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_article_data(blog,'content')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9.  Ask yourself:**\n",
    "\n",
    "- **If your corpus is 493KB, would you prefer to use stemmed or lemmatized text?**\n",
    "\n",
    "lemmatized -- it takes longer but is better quality -- and with such little data, shouldn't add too much time\n",
    "\n",
    "\n",
    "- **If your corpus is 25MB, would you prefer to use stemmed or lemmatized text?**\n",
    "\n",
    "lemmatized -- it takes longer but is better quality -- and with such little data, shouldn't add too much time\n",
    "\n",
    "\n",
    "- **If your corpus is 200TB of text and you're charged by the megabyte for your hosted computational resources, would you prefer to use stemmed or lemmatized text?**\n",
    "\n",
    "Stem, because it runs faster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
